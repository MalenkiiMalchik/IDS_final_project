---
title: "Pushshift.io API"
author: "Alex Richardson"
date: "10/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(reticulate)
require(rvest)
require(tidyverse)
require(here)
require(skimr)
require(lubridate)
require(tidytext)
require(reshape2)
require(topicmodels)
require(quanteda)
require(tictoc)
require(recipes)
require(caret)
require(ranger)
```

```{r Scraping usernames and karmas}

 raw = read_html("https://www.reddit.com/wiki/suspiciousaccounts")
table = raw %>% 
  html_nodes(.,xpath="/html/body/div[1]/div/div/div/div[2]/div/div/div/div[2]/div[3]/div[1]/div[2]/div/div[1]/div/table") %>% 
  html_table()
table = data.frame(table)
```

```{r Getting rid of "/u" and making a list of usernames}
usernamelist = c()
for(n in 1:length(table$Username)){
  table$Username[n] = str_remove(table$Username[n], "u/")
  usernamelist = append(usernamelist, table$Username[n])
}

write_csv(table, "table-of-usernames.csv")

table
```


```{python setup}
import psaw
import datetime as dt

from psaw import PushshiftAPI

api = PushshiftAPI()

```


```{python Pulling in Threadlist}

pyusernamelist = r.usernamelist

threadlist = []
for n in pyusernamelist:
  templist = list(api.search_submissions(
                            author=n,
                            filter=['url','author', 'title', 'subreddit', 'id']
                            ))
  threadlist.append(templist)
  

  
```

```{r Cleaning Threadlist a Bit}
threadlist = py$threadlist
cleaner_threadlist = c()
for (n in threadlist){
  for (r in n){
      cleaner_threadlist = append(cleaner_threadlist, r)
  }
}
```

```{r Cleaning them the rest of the way}
cleanest_threadlist = c()
author = c()
date = c()
subreddit = c()
title = c()
url = c()
id = c()
tholder = c()
aholder = c()
dholder = c()
uholder = c()
sholder = c()
iholder = c()
for (o in cleaner_threadlist){
  author = as.character(o$author)
  date = as.character(o$created)
  title = as.character(o$title)
  subreddit = as.character(o$subreddit)
  url = as.character(o$url)
  id = as.character(o$id)
  tholder = append(tholder, title)
  aholder = append(aholder, author)
  dholder = append(dholder, date)
  uholder = append(uholder, url)
  sholder = append(sholder, subreddit)
  iholder = append(iholder, id)
}
tholder = as.data.frame(tholder)
aholder = as.data.frame(aholder)
dholder = as.data.frame(dholder)
uholder = as.data.frame(uholder)
sholder = as.data.frame(sholder)
iholder = as.data.frame(iholder)

cleanest_threadlist = bind_cols(tholder, aholder, dholder, uholder, sholder, iholder)
cleanest_threadlist = rename(cleanest_threadlist, "Title" = tholder, "Author" = aholder, "Date" = dholder, "URL" = uholder, "Subreddit" = sholder, "ID" = iholder)
head(cleanest_threadlist)

idlist = as.character(cleanest_threadlist$ID)
write_csv(cleanest_threadlist, here("Threadlist.csv"))
```

```{r Just importing something to deal with the python problem...}
idlist = read_csv("propthreads.csv")

idlist = idlist %>% select("Index" = "X1", "Title" = "0", "Author" = "0_1", "Created" = "0_2", "Link" = "0_3", "Subreddit" = "0_4", "id" = "0_5", "num_comments" = "0_6") %>% mutate(Created = as_datetime(Created))

#arrange(idlist, desc(num_comments))

summarize(idlist, tot_comments = sum(num_comments))

#head(idlist)

```


```{python Pulling in Comment Author Gens}
pyidlist = r.idlist

x = 0
comment_author_list = []
for n in pyidlist:
  print(n)
  temp2list = api.search_comments(
                            id=n,
                            filter=['author','id','subreddit','title']
                            )
  print(x)
  x += 1
  comment_author_list.append(temp2list)

```

```{python Pulling in Comment Author List}
caches_list = []
max_response_cache = 100
cache = []
for c in comment_author_list:
    for d in c:
      cache.append(d)
      if len(cache) >= max_response_cache:
        break
    caches_list.append(cache)
```


```{r Cleaning Comment Author List}
final_cal = c()
author = c()
date = c()
subreddit = c()
title = c()
id = c()
tholder = c()
aholder = c()
dholder = c()
sholder = c()
iholder = c()
for (n in caches_list){
  for (o in n){
    author = as.character(o$author)
    date = as.character(o$created)
    subreddit = as.character(o$subreddit)
    id = as.character(o$id)
    aholder = append(aholder, author)
    dholder = append(dholder, date)
    sholder = append(sholder, subreddit)
    iholder = append(iholder, id)
  }
}
aholder = as.data.frame(aholder)
dholder = as.data.frame(dholder)
sholder = as.data.frame(sholder)
iholder = as.data.frame(iholder)

final_cal = bind_cols(tholder, aholder, dholder, sholder, iholder)
final_cal = rename(final_cal, "Author" = aholder, "Date" = dholder, "Subreddit" = sholder, "ID" = iholder)
head(final_cal)

write_csv(final_cal, here("Comment-Author-List.csv"))

```

```{r Grabbing Unique Authors}

final_cal <- read_csv("Comment-Author-List.csv")

final_cal = final_cal %>% mutate(AuthDup = as.numeric(duplicated(Author))) %>% filter(AuthDup==0)
comment_author_list = final_cal$Author

write_csv(final_cal, "Comment-Author-List-Clean.csv")

```

### Note: I had to manually clean out the 'users' [Deleted] and AutoModerator, for obvious reasons. Also, for reasons that are still an abject mystery, I had to run the code below in Spyder because RStudio seems to have lost track of Python.

```{python Pulling in Comment Gens}
start_epoch=int(dt.datetime(2016, 1, 1).timestamp())
end_epoch=int(dt.datetime(2016, 12, 31).timestamp())

py_comm_auth_list = r.comment_author_list

x = 0
comment_history_gen = []
for n in py_comm_auth_list:
  print(n)
  temp3list = api.search_comments(after=start_epoch,
                            before=end_epoch,
                            author=n,
                            )
  print(x)
  x += 1
  comment_history_gen.append(temp3list)

```

```{python Pulling in Comments}
hist_caches_list = []
hist_cache = []
for c in comment_history_gen:
    for d in c:
      hist_cache.append(d)
    hist_caches_list.append(hist_cache)
    
```

```{python Assembling Comments into a DataFrame}

dataset = pd.DataFrame()
x = 0
for n in hist_caches_list:
    for o in n:
        dataset = dataset.append(pd.DataFrame(o.d_, index=[x]))
        x+=1
        
dataset.to_csv('dataset.csv')

```

```{r}

dataset = read_csv('dataset.csv')

head(dataset)

```

```{r}
user_comments <- dataset %>% mutate(created = as_datetime(created), created_utc = as_datetime(created_utc), author_created_utc = as_datetime(author_created_utc), edited = as_datetime(edited)) # Cleaning Date-Times

user_comments <- user_comments %>% mutate(time_to_post = created_utc - author_created_utc, timezone = created_utc-created)

user_comments <- user_comments %>% mutate(propthread = )

skim(user_comments)
```

```{r Cleaning and Wording}
user_words = user_comments %>% group_by(author) %>%  unnest_tokens(word,body,token = "words") %>% anti_join(stop_words) %>%  mutate(word = SnowballC::wordStem(word)) 

user_words = user_words %>% filter(!str_detect(word,"http")) %>% filter(!str_detect(word,"www")) %>% filter(!str_detect(word,"gt")) %>% filter(!str_detect(word,"redd.it")) %>% filter(!str_detect(word,"\\d")) %>% filter(!str_detect(word,".com")) %>% filter(!str_detect(word,"_"))

```

```{r Put this somethere else!!!}

user_comments = user_comments %>% mutate(clean_id = str_sub(link_id,-6,-1))


user_comments = user_comments %>% mutate(propaganda_thread = 0)

x = 0
for (n in user_comments$clean_id){
  print(x)
  x = x+1
  if((n %in% idlist$id)==T){
    user_comments[x,"propaganda_thread"]=1 
  }
}

summarize(user_comments, propaganda_count = sum(propaganda_thread))

```

```{r User-Word Frequency}

user_counts = user_words %>%  count(word,sort=T)

user_counts2 <- 
  user_counts %>% 
  bind_tf_idf(word, author, n)

user_counts2 %>% 
  select(n,tf,idf,tf_idf)

```

```{r Trying a Visualization}

user_counts3 = user_counts2 %>% filter(author == c("mikepictor", "Bossman1086", "TossMeAwayToTheMount"))

user_counts3 %>%
  group_by(author) %>% 
  top_n(5, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf)) +
  geom_col(show.legend = F) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~author,ncol=1,scales="free") +
  theme(text=element_text(size=10))

```
```{r User-Word Frequency Subreddit}

user_words = user_words %>% ungroup() %>% group_by(subreddit)

user_counts = user_words %>%  count(word,sort=T)

user_counts4 <- 
  user_counts %>% 
  bind_tf_idf(word, subreddit, n)

user_counts4 %>% 
  select(n,tf,idf,tf_idf)

```

```{r Trying a subreddit Visualization}

user_counts4 = user_counts4 %>% filter(subreddit == c("news", "politics", "worldnews"))

user_counts4 %>%
  group_by(subreddit) %>% 
  top_n(5, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf)) +
  geom_col(show.legend = F) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~subreddit,ncol=1,scales="free") +
  theme(text=element_text(size=10))

```

```{r}
comments_corpus <- user_words %>% count(author, word) %>% cast_dtm(author,word,n)


comments_lda <- LDA(comments_corpus, k = 5, control = list(seed = 1989))


author_topics <- tidy(comments_lda, matrix = "beta")


comments_top_terms <- 
  author_topics %>%
  group_by(topic) %>% # Group by the topics
  # Grab the top 10 words most 
  # associated with the topic
  top_n(10, beta) %>% 
  ungroup() %>% # Ungroup
  arrange(topic, -beta) # Arrange 
comments_top_terms
```

```{r}
comments_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  theme(text=element_text(size=10))
```

```{r}
comments_corpus_pol <- user_words %>% ungroup() %>% filter((subreddit == "politics")==T | (subreddit == "news")==T | (subreddit == "worldnews")==T) %>% count(author, word) %>% cast_dtm(author,word,n)


comments_lda_pol <- LDA(comments_corpus_pol, k = 6, control = list(seed = 1989))


author_topics_pol <- tidy(comments_lda_pol, matrix = "beta")


comments_top_terms_pol <- 
  author_topics_pol %>%
  group_by(topic) %>% # Group by the topics
  # Grab the top 10 words most 
  # associated with the topic
  top_n(10, beta) %>% 
  ungroup() %>% # Ungroup
  arrange(topic, -beta) # Arrange 
write_csv(comments_top_terms_pol, "comments_top_terms_pol.csv")
```

```{r}
pol_topics <- comments_top_terms_pol %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  theme(text=element_text(size=10))

ggsave(plot = pol_topics, filename = "pol_topics.png", device = "png")
```

```{r Sorting by Political Subreddits}

filtered_subreddits = user_comments %>% filter((str_detect(subreddit, "polit")) | (str_detect(subreddit, "trump")) | (str_detect(subreddit, "donald")) | (str_detect(subreddit, "hillary")) | (str_detect(subreddit, "clinton")) | (str_detect(subreddit, "news")) | (str_detect(subreddit, "world")) | (str_detect(subreddit, "sanders")) | (str_detect(subreddit, "bernie")) | (str_detect(subreddit, "gop")) | (str_detect(subreddit, "repub")) | (str_detect(subreddit, "democrat")))

```

```{r Cleaning and Wording Pol}
user_words_pol = filtered_subreddits %>% group_by(author) %>%  unnest_tokens(word,body,token = "words") %>% anti_join(stop_words) %>%  mutate(word = SnowballC::wordStem(word)) 

user_words_pol = user_words_pol %>% filter(!str_detect(word,"http")) %>% filter(!str_detect(word,"www")) %>% filter(!str_detect(word,"gt")) %>% filter(!str_detect(word,"redd.it")) %>% filter(!str_detect(word,"\\d")) %>% filter(!str_detect(word,".com")) %>% filter(!str_detect(word,"_")) 

```

```{r User-Word Frequency}

user_counts_pol = user_words_pol %>%  count(word,sort=T)

user_counts_pol2 <- 
  user_counts_pol %>% 
  bind_tf_idf(word, author, n)

user_counts_pol2 %>% 
  select(n,tf,idf,tf_idf)

```

```{r Trying a Visualization}

user_counts_pol3 = user_counts_pol2 %>% filter((author == "mikepictor") | (author == "ChefBoyAreWeFucked") | (author == "RyuNoKami"))

user_counts_pol3 %>%
  group_by(author) %>% 
  top_n(5, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf)) +
  geom_col(show.legend = F) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~author,scales="free") +
  theme(text=element_text(size=10))

```

```{r}
comments_corpus_pol <- user_words_pol %>% count(author, word) %>% cast_dtm(author,word,n)


comments_lda_pol <- LDA(comments_corpus_pol, k = 8, control = list(seed = 1989))


author_topics_pol <- tidy(comments_lda_pol, matrix = "beta")


comments_top_terms_pol <- 
  author_topics_pol %>%
  group_by(topic) %>% # Group by the topics
  # Grab the top 10 words most 
  # associated with the topic
  top_n(10, beta) %>% 
  ungroup() %>% # Ungroup
  arrange(topic, -beta) # Arrange 
comments_top_terms_pol
```

```{r}
comments_top_terms_pol %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  theme(text=element_text(size=10))
```

```{r Mentions Candidate}

can_comments = user_comments %>% filter((str_detect(body, "trump")) | (str_detect(body, "donald")) | (str_detect(body, "hillary")) | (str_detect(body, "clinton")) | (str_detect(body, "sanders")) | (str_detect(body, "bernie")) | (str_detect(body, "Trump")) | (str_detect(body, "Donald")) | (str_detect(body, "Hillary")) | (str_detect(body, "Clinton")) | (str_detect(body, "Sanders")) | (str_detect(body, "Bernie")))

```

```{r Marking Candidate Mention}

can_comments = can_comments %>% mutate(mention = case_when(
         str_detect(body, "bernie") ~ "Bernie",
         str_detect(body, "Bernie") ~ "Bernie",
         str_detect(body, "sanders") ~ "Bernie",
         str_detect(body, "Sanders") ~ "Bernie",
         str_detect(body, "hillary") ~ "Hillary",
         str_detect(body, "Hillary") ~ "Hillary",
         str_detect(body, "clinton") ~ "Hillary",
         str_detect(body, "Clinton") ~ "Hillary",
         str_detect(body, "donald") ~ "Donald",
         str_detect(body, "Donald") ~ "Donald",
         str_detect(body, "trump") ~ "Donald",
         str_detect(body, "Trump") ~ "Donald"
         ))

```


```{r Cleaning and Wording Candidate Mentions}
user_words_can = can_comments %>% group_by(author, mention) %>%  unnest_tokens(word,body,token = "words") %>% anti_join(stop_words) %>%  mutate(word = SnowballC::wordStem(word)) 

user_words_can = user_words_can %>% filter(!str_detect(word,"http")) %>% filter(!str_detect(word,"www")) %>% filter(!str_detect(word,"gt")) %>% filter(!str_detect(word,"redd.it")) %>% filter(!str_detect(word,"\\d")) %>% filter(!str_detect(word,".com")) %>% filter(!str_detect(word,"_"))

```

```{r Applying affinity}
sent_dict <- get_sentiments("afinn")
sent_text <- user_words_can %>% inner_join(sent_dict) 
sent_text <- sent_text %>% group_by(author, mention)
sum_sent_text <- sent_text %>% summarize(Average_Sentiment = mean(value))
sum_sent_text

```

```{r}

sum_sent_text = sum_sent_text %>% spread(mention, Average_Sentiment) 
sum_sent_text[is.na(sum_sent_text)] <- 0
sum_sent_text = sum_sent_text %>% mutate(Democrat = Bernie+Hillary-Donald)

sum_sent_text = sum_sent_text %>% select(author, Democrat)
```

```{r}
can_comments_sent = can_comments %>% full_join(sum_sent_text)
```


```{r}
can_comments_sent = can_comments_sent %>% mutate(day = round_date(created_utc, unit = "day")) %>% group_by(author, day)
times_per_day = can_comments_sent %>% summarize(times_per_day = n())

times_per_day = times_per_day %>% group_by(author) %>% summarise(num_pol_posts = sum(times_per_day))

can_comments_times = can_comments_sent %>% full_join(times_per_day)
```

```{r}
prepset = can_comments_times %>% ungroup() %>% select(author, author_created_utc, controversiality, created_utc, gilded, nest_level, reply_delay, score, time_to_post, mention, Democrat, num_pol_posts,subreddit) 

rcp1 = recipe(Democrat~.,prepset) %>% 
  step_dummy(all_nominal(),-author) %>%
  prep()

prepset <- bake(rcp1, prepset)

skim(prepset)

lazyholder = c()
for (n in variable.names(prepset)[12:168]){
  lazyholder = paste0(lazyholder, n," = sum(", n, "), ")
}



prepset2 = prepset %>% group_by(author) 

prepset2 = prepset2 %>% summarise(author_created_utc = mean(author_created_utc), controversiality = mean(controversiality), created_utc = mean(created_utc), gilded = sum(gilded), nest_level = mean(nest_level), reply_delay = mean(reply_delay), score = mean(score), time_to_post = mean(time_to_post), Democrat = mean(Democrat), num_pol_posts = mean(num_pol_posts), mention_Donald = sum(mention_Donald), mention_Hillary = sum(mention_Hillary), subreddit_AskAnAmerican = sum(subreddit_AskAnAmerican), subreddit_AskReddit = sum(subreddit_AskReddit), subreddit_CapitalismVSocialism = sum(subreddit_CapitalismVSocialism), subreddit_Conservative = sum(subreddit_Conservative), subreddit_conspiracy = sum(subreddit_conspiracy), subreddit_Libertarian = sum(subreddit_Libertarian),  subreddit_news = sum(subreddit_news), subreddit_politics = sum(subreddit_politics), subreddit_SandersForPresident = sum(subreddit_SandersForPresident), subreddit_ShitRConservativeSays = sum(subreddit_ShitRConservativeSays), subreddit_The_Donald = sum(subreddit_The_Donald), subreddit_worldnews = sum(subreddit_worldnews))

prepset2 = ungroup(prepset2)

prepset2[is.na(prepset2)] <- 0


skim(prepset2)

```


```{r}
set.seed(1989)
index = createDataPartition(prepset2$Democrat,p=.8,list=F) 
train_data = prepset2[index,] # Use 80% of the data as training data 
test_data = prepset2[-index,] # holdout 20% as test data 

dim(train_data)
```

```{r}
rcp <- 
  recipe(Democrat~.,train_data) %>% 
  step_range(all_numeric()) %>%  # Normalize scale
  prep()

train_data2 <- bake(rcp,train_data)
test_data2 <- bake(rcp,test_data)
```

```{r}
train_data2 <- train_data2 %>% select(-subreddit_AskAnAmerican)
test_data2 <- test_data2 %>% select(-subreddit_AskAnAmerican)

write_csv(train_data2, "train_data2.csv")
```


```{r}
set.seed(1989) 

folds <- createFolds(train_data2$Democrat, k = 5) 

sapply(folds,length)
```

```{r}
control_conditions <- 
  trainControl(method='cv', 
               index = folds,
               classProbs = TRUE
  )
```

```{r Random Forest}
mod_rf <-
  train(Democrat ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "ranger", # random forest (ranger is much faster than rf)
        metric = "RMSE",     # mean squared error
        trControl = control_conditions,
  )
```

```{r KNN}
knn_tune = expand.grid(k = c(1,3,5,7))
mod_knn <-
  train(Democrat ~ .,           # Equation (outcome and everything else)
        data=train_data2,  # Training data 
        method = "knn",    # K-Nearest Neighbors Algorithm
        metric = "RMSE",   # mean squared error
        trControl = control_conditions, # Cross validation conditions
        tuneGrid = knn_tune # Vary the tuning parameter K 
  )
```

```{r linear}
mod_lm <-
  train(Democrat ~ .,          # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "lm",    # linear model
        metric = "RMSE",   # mean squared error
        trControl = control_conditions # Cross validation conditions
  )
```

```{r}
mod_list <-
  list(
    lm = mod_lm,
    knn = mod_knn,
    rf = mod_rf 
  )
dotplot(resamples(mod_list),metric = "RMSE")

```



```{r}
pred <- predict(mod_rf,newdata = test_data2)
mse = sum(test_data2$Democrat-pred^2)/nrow(test_data2)
mse 
```

## Using tf_idf on candidate mentions!


```{r Cleaning and Wording}
can_words = can_comments %>% group_by(mention) %>%  unnest_tokens(word,body,token = "words") %>% anti_join(stop_words) %>%  mutate(word = SnowballC::wordStem(word)) 

can_words = can_words %>% filter(!str_detect(word,"http")) %>% filter(!str_detect(word,"www")) %>% filter(!str_detect(word,"gt")) %>% filter(!str_detect(word,"redd.it")) %>% filter(!str_detect(word,"\\d")) %>% filter(!str_detect(word,".com")) %>% filter(!str_detect(word,"_")) %>% filter(!str_detect(word,"donald")) %>% filter(!str_detect(word,"trump")) %>% filter(!str_detect(word,"hillary")) %>% filter(!str_detect(word,"clinton")) %>% filter(!str_detect(word,"berni")) %>% filter(!str_detect(word,"sander")) %>% filter(!str_detect(word,"Donald")) %>% filter(!str_detect(word,"Trump")) %>% filter(!str_detect(word,"Hillary")) %>% filter(!str_detect(word,"Clinton")) %>% filter(!str_detect(word,"Berni")) %>% filter(!str_detect(word,"Sander")) %>% filter(!str_detect(word,"onjectid")) 


```


```{r User-Word Frequency}

can_counts = can_words %>%  count(X1, word) %>% cast_dtm(document = X1, term = word, value = n,
           weighting = tm::weightTfIdf)


can_counts2 <- 
  can_counts %>% 
  bind_tf_idf(word, mention, n)

can_counts2 %>% 
  select(n,tf,idf,tf_idf)

```

```{r Trying a Visualization}

can_counts2 %>%
  group_by(mention) %>% 
  top_n(10, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf)) +
  geom_col(show.legend = F) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~mention, ncol=1,scales="free") +
  theme(text=element_text(size=10))

```

```{r}
set.seed(1989) 

folds <- createFolds(can_comments$mention, k = 5) 

sapply(folds,length)
```

```{r}
control_conditions <- 
  trainControl(method='cv', 
               index = folds,
               classProbs = TRUE
  )
```

```{r}
mention_rf <- train(x = as.matrix(can_counts),
                     y = factor(can_comments$mention),
                     method = "ranger",
                     metric = "ROC",
                     trControl = control_conditions
)
```

